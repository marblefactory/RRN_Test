{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRN-LSTM with Tensorflow\n",
    "\n",
    "Following the tutorial [A noob’s guide to implementing RNN-LSTM using Tensorflow](http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow).\n",
    "\n",
    "See [Andrej Karpathy's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) for more information on RRNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Given a binary string, containing `0`s and `1`s, of length 20, we need to determine the count of `1`s the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Solution\n",
    "This is easy to solve without using a RRN, as shown in the code below. However, this is a simple problem with which to experiment using RRNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def count_num_1s(s: str) -> int:    \n",
    "    return reduce((lambda acc, x: acc + x if x == 1 else acc), s)\n",
    "\n",
    "# Should print out '9'.\n",
    "print(str(count_num_1s([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRN Solution\n",
    "\n",
    "## All Possible Data\n",
    "\n",
    "- Each input is a binary string of length 10.\n",
    "- An input will be represented as a python list of `0`s and `1`s.\n",
    "- There are $2^{10}$ possible combinations of `0`s and `1`s. \n",
    "- The network will be trained on a subset all possible combinations.\n",
    "- The remainder of the data will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 1], [1, 0], [1, 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def make_all_binary_strings(length: int = 10) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    returns: a list of all possible binary strings, where a binary string is represened as a list of ints.\n",
    "    \"\"\"\n",
    "    format_string = '{0:0' + str(length) + 'b}'\n",
    "    all_binary_strings = [format_string.format(i) for i in range(2**length)]\n",
    "    return [list(map(int,i)) for i in all_binary_strings]\n",
    "\n",
    "example_all_arrs = make_all_binary_strings(length=2)\n",
    "print(example_all_arrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow requires input as a tensor, i.e. a Tensorflow variable.\n",
    "- The tensor is 3D with the dimensions [batch_size, sequence_length, input_dimension], where\n",
    "    - `batch_size` is something we’ll determine later \n",
    "    - `sequence_length` is fixed at 10 \n",
    "    - `input_dimension` is 1 i.e each individual bit of the string. Therefore, each bit will actually be represented as a list containing just that bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0],\n",
      "       [0]]), array([[0],\n",
      "       [1]]), array([[1],\n",
      "       [0]]), array([[1],\n",
      "       [1]])]\n"
     ]
    }
   ],
   "source": [
    "def make_all(binary_strings: List[List[int]]) -> List[np.array]:\n",
    "    # Turn every every element into an array containing that one element.\n",
    "    # This is required as the input_dimension is 1.\n",
    "    xs = list(map(lambda bits: list(map(lambda bit: [bit], bits)), binary_strings))\n",
    "    # Make each array of bits a numpy array.\n",
    "    return list(map(lambda bits: np.array(bits), xs))\n",
    "\n",
    "example_all = make_all(example_all_arrs)\n",
    "print(example_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling Training Data\n",
    "- For every sequence, the result can be anything between 0 and 20. \n",
    "- Therefore there are have 21 categories in which a sequence can be classified into.\n",
    "- Each sequence belongs to the class number which is the same as the count of ones in the sequence.\n",
    "- The output is represented using 1-hot encoding.\n",
    "- i.e. a vector containing all zeros except of one position which containings a one, indicating the class the sequence belongs to.\n",
    "\n",
    "For example, the sequence `0011` would have the top output vector below, indicating there are two `1`s in the sequence.\n",
    "\n",
    "`[0 0 1 0 0]`\n",
    "\n",
    "`[0 1 2 3 4]` (the number of `1`s)\n",
    "\n",
    "The length of the output vector is the length of the binary sequence + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0, 0], [1, 0, 0]), ([0, 1], [0, 1, 0]), ([1, 0], [0, 1, 0]), ([1, 1], [0, 0, 1])]\n"
     ]
    }
   ],
   "source": [
    "def label(data: List[np.array]) -> List[int]:\n",
    "    def f(bits: np.array):\n",
    "        # We made each element a single array in `make_training`.\n",
    "        # By flattening we can more easily work with the data.\n",
    "        flattened = bits.flatten()\n",
    "        # The index in the one-hot vector that should be set to 1.\n",
    "        i = count_num_1s(flattened)\n",
    "        # Create the vector and set the class.\n",
    "        vec = [0] * (len(bits) + 1)\n",
    "        vec[i] = 1\n",
    "        return vec\n",
    "    \n",
    "    return list(map(f, data))\n",
    "\n",
    "example_labels = label(example_all)\n",
    "print(list(zip(example_all_arrs, example_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Data\n",
    "- The full data set is split into training and testing data.\n",
    "- These sets don't overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: [array([[1],\n",
      "       [1]]), array([[0],\n",
      "       [1]]), array([[1],\n",
      "       [0]])]\n",
      "\n",
      "Testing: [array([[0],\n",
      "       [0]])]\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from random import shuffle\n",
    "\n",
    "def split(data: List[np.array], num_training: int) -> Tuple[List[np.array], List[np.array]]:\n",
    "    \"\"\"\n",
    "    data: a list of the data to be split into training and testing.\n",
    "    proportion_testing: the proportion in which the data will be split in two. \n",
    "    returns: shuffled data split into training and testing data.\n",
    "    \"\"\"    \n",
    "    # shuffle works in place, therefore copy the data.\n",
    "    shuffled_data = list(data)\n",
    "    shuffle(shuffled_data)\n",
    "    \n",
    "    training = shuffled_data[:num_training]\n",
    "    testing  = shuffled_data[num_training:]\n",
    "    \n",
    "    return (training, testing)\n",
    "\n",
    "example_training, example_testing = split(example_all, num_training=3)\n",
    "\n",
    "print(\"Training: \" + str(example_training))\n",
    "print(\"\\nTesting: \" + str(example_testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Model\n",
    "\n",
    "- Define two variables for data.\n",
    "- The dimensions for the data are $[\\text{Batch Size}, \\text{Sequence Length}, \\text{Input Dimension}]$. \n",
    "- `training_data` has sequence length of 10 and input dimension of 1, as defined earlier.\n",
    "- `training_labels` has sequence length of 11 for the one-hot encoding representing all the possible classes (0, 1, 2, etc)\n",
    "- The batch size is determined at runtime, and hence is `None`.\n",
    "- Placeholders tell Tensorflow that the data will be supplied later.\n",
    "- Build the model first, then run it right at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sequence_length = 16\n",
    "data = tf.placeholder(tf.float32, [None, sequence_length, 1])\n",
    "target = tf.placeholder(tf.float32, [None, sequence_length + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell\n",
    "- For each LSTM cell that we initialise, we need to supply a value for the hidden dimension\n",
    "- i.e. the number of units in the LSTM cell. \n",
    "- If the value os too high a value may lead to overfitting\n",
    "- If the value is too low value may yield poor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 24\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_hidden,state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "- Unroll the network, pass data to it, and store the value in `val`.\n",
    "- `state` is discarded (never used again).\n",
    "- This doesn't run the NN.\n",
    "- These represent functions that are stored in variables.\n",
    "- The functions are run when a session is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Labels\n",
    "- Transpose to switch batch size and sequence size and store the result in `val`.\n",
    "- This is done so we can get the output at the sequence's last input.\n",
    "- i.e. in a string of 10 bits we're only interested in the output (class) we got at the 20th character.\n",
    "- Class labels are generated as we're going along because we're using a RNN (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "In order to apply the final transformation to the outputs of the LSTM and map it to the output classes (i.e. number of `1`s), we need the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([num_hidden, int(target.get_shape()[1])]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[target.get_shape()[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.matmul(last, weight) + bias` gives matrix with a variety of values for each class.\n",
    "- i.e. the class for each sequence (?)\n",
    "- We are interested in the probability score for each class.\n",
    "- The value represents the probablilty the sequence belongs to a particular class.\n",
    "- i.e the chance that the sequence belongs to a particular class\n",
    "- This is done using the softmax activation function.\n",
    "- This function takes in a vector of values and returns a probability distribution for each index depending upon its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss (Correctness)\n",
    "\n",
    "- Calculate the loss.\n",
    "- i.e. the degree of incorrectness.\n",
    "- The cost function determines how poorly or how well our predictions stack against the actual results (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(target * tf.log(tf.clip_by_value(prediction,1e-10,1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "- Use an optimisation function to optimise the network.\n",
    "- i.e. set weights on edges (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albiebakersmith/anaconda3/envs/rrn_test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Testing \n",
    "- The below error is a count of how many sequences in the test dataset were classified incorrectly. \n",
    "- This gives us an idea of the correctness of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction, 1))\n",
    "error = tf.reduce_mean(tf.cast(mistakes, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "Runs the functions created before to generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training = 10000\n",
      "Num testing  = 55536\n"
     ]
    }
   ],
   "source": [
    "all_arrs = make_all_binary_strings(length=sequence_length)\n",
    "all_data = make_all(all_arrs)\n",
    "\n",
    "num_training = 10000#int(len(all_data) * 0.1)\n",
    "print(\"Num training = \" + str(num_training))\n",
    "print(\"Num testing  = \" + str(len(all_data) - num_training))\n",
    "\n",
    "train_input, test_input = split(all_data, num_training=15)\n",
    "\n",
    "train_output = label(train_input)\n",
    "test_output = label(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "no_of_batches = int(len(train_input)/batch_size)\n",
    "epoch = 5000\n",
    "for i in range(epoch):\n",
    "    ptr = 0\n",
    "    for j in range(no_of_batches):\n",
    "        inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]\n",
    "        ptr+=batch_size\n",
    "        sess.run(minimize,{data: inp, target: out})\n",
    "#     print(\"Epoch - \", str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000 error 98.6%\n"
     ]
    }
   ],
   "source": [
    "incorrect = sess.run(error,{data: test_input, target: test_output})\n",
    "print('Epoch {:2d} error {:3.1f}%'.format(i + 1, 100 * incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02869153  0.08481354  0.03939343  0.04407435  0.03804926  0.06192828\n",
      "   0.0629071   0.07557759  0.04023664  0.06145936  0.03298585  0.07052813\n",
      "   0.03663613  0.09131815  0.04738227  0.07781862  0.10619969]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(prediction,{data: [[[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],[1],[0],[0],[1],[1],[0]]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
